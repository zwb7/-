1. 在做区分的任务或者域偏移较大任务上之前的方法比较不那么令人满意。例子是：CoGANs。这种方法只在源域和目标域非常相似的情况下(如MNIST和USPS)显示出优势，在实验中，很难使它收敛到更大的分布偏移。
2. 在能够处理大偏差任务的方法上，大多采用了固定权重的方法来进行训练（主要就是target domain的分类那边，基本是源和目标共用一个分类器）。
3. 其实很多方法都没有用到GAN的标准的loss。

模型特点：结合了判别模型，无约束权重共享，GAN loss。
优点：比竞争性的domain adversarial方法更加简单。


论文想解决三个问题：
是使用一个基于生成器还是鉴别器的模型
是使用固定的参数还是不固定的参数（对称的还是非对称的）
loss 函数怎么选


首先：使用含标签的源图像训练源域上的CNN和一个分类器。
然后：通过对抗的方法训练一个用于目标域的CNN和一个判别器，判别输入来自源域还是目标域，从而达到对抗的效果（打个比方：现在有个判断动物是否有尾巴的模型，source是马，target是猪，这个网络就是希望把它们“尾巴”的共同特征找到，而不是把短尾当没有）。
测试中：目标域上的CNN和一开始的分类器就可以用来对目标域上的图像进行分类。虚线表明这是固定的网络参数（意思是直接套用的）。

关于pre-training：
1.为什么要无约束权值共享？这是一个灵活的学习模式，能学习到更多领域特征。
2.为什么要保留一部分权值？有可能产生退化解。
3.怎么解决？把对source预训练出的模型作为target表达空间初始版本再通过训练去改进，并在对抗训练时固定源模型。


损失函数如上
第一个式子训练一个classifier C和用于source上的CNN模型Mt,用的是标准的supervise损失。E是数学期望，这里是离散分布所以就是取个均值。1是因为domain discriminator的输出在0和1之间，source的label为1，target的label为0，discriminator希望让target数据的输出D(Mt(xt))尽量靠近0，这样LadvD的loss就能最小化。
第二个式子训练一个用于区分source和target数据的discriminator D，我们希望最小化该损失，即希望训练的判别器尽可能准确地分辨出输入来自源域还是目标域。
具体操作就是：
通过source上的sample和label训练 Ms 和 C 。
然后我们保持Ms不变，用 Ms 初始化 Mt ，然后通过优化第二项和第三项得到最终的 D 和 Mt 。
最终就可以直接使用上面训练的 Mt 和 C 对目标域上的样本进行分类。


DANN中的方法是利用梯度反转层来最大化判别器D的损失，但是这个损失函数存在问题：训练开始时判别器收敛的很快导致梯度消失。
还有就是GAN中的损失函数，相当于有两个独立的损失函数，一个是生成器的，另一个是判别器的。这个损失函数虽然有更强的梯度，但是当两个分布都变化时，这个损失函数会导致振荡。
Simultaneous Deep Transfer Across Domains and Tasks中使用的是交叉熵损失。这个方法呢，就综合考虑了两个域相互接近的过程，比较稳定。
但因为我们使用pre-train的源模型作为目标表示空间的初始化，并在对抗性训练中修正源模型。这样做，我们有效地学习了一个非对称映射，在这个映射中，我们修改目标模型以匹配源分布。这与原始的GAN环境最相似，生成的空间被更新，直到与固定的实际空间无法区分为止。因此，我们选择inverted label GAN loss。
